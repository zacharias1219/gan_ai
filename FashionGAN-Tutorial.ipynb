{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f04b6a-fe54-41ce-92fe-ae94fc587387",
   "metadata": {
    "id": "64f04b6a-fe54-41ce-92fe-ae94fc587387"
   },
   "source": [
    "# 1. Import Dependencies and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c355f3-39f9-4f73-ac4e-5ad62198a580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install tensorflow tensorflow-gpu matplotlib tensorflow-datasets ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6112401-2397-423d-b8c2-113a9d323ab0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84be907-35e2-43db-a645-b6b164302aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bringing in tensorflow\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f2aa32-064b-448c-bb27-f19a48c40115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brining in tensorflow datasets for fashion mnist \n",
    "import tensorflow_datasets as tfds\n",
    "# Bringing in matplotlib for viz stuff\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c933f988-d1ee-4d4d-8028-368a158c27e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the tensorflow datasets api to bring in the data source\n",
    "ds = tfds.load('fashion_mnist', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c361db0d-8e7b-43e1-97f9-5e3f7cb01ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.as_numpy_iterator().next()['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1635e4-4beb-493d-92c1-b106c806ca70",
   "metadata": {
    "id": "ea1635e4-4beb-493d-92c1-b106c806ca70"
   },
   "source": [
    "# 2. Viz Data and Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c62caf-e406-4d12-af31-6f4848155844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some data transformation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3215c900-6e85-4b39-b300-ea18faf30e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup connection aka iterator\n",
    "dataiterator = ds.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d6e079-46da-43ca-80d2-b3c864d90360",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Getting data out of the pipeline\n",
    "dataiterator.next()['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb5fca0-fd8a-4557-9c72-1a60c289a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the subplot formatting \n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "# Loop four times and get images \n",
    "for idx in range(4): \n",
    "    # Grab an image and label\n",
    "    sample = dataiterator.next()\n",
    "    # Plot the image using a specific subplot \n",
    "    ax[idx].imshow(np.squeeze(sample['image']))\n",
    "    # Appending the image label as the plot title \n",
    "    ax[idx].title.set_text(sample['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c9d901-6a5c-42fd-ad06-cc03f7829728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and return images only \n",
    "def scale_images(data): \n",
    "    image = data['image']\n",
    "    return image / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc9b6b1-e06e-421c-9c5c-bfc3b3e3be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the dataset \n",
    "ds = tfds.load('fashion_mnist', split='train')\n",
    "# Running the dataset through the scale_images preprocessing step\n",
    "ds = ds.map(scale_images) \n",
    "# Cache the dataset for that batch \n",
    "ds = ds.cache()\n",
    "# Shuffle it up \n",
    "ds = ds.shuffle(60000)\n",
    "# Batch into 128 images per sample\n",
    "ds = ds.batch(128)\n",
    "# Reduces the likelihood of bottlenecking \n",
    "ds = ds.prefetch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb52952-faa1-445f-8931-2f0f37224bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.as_numpy_iterator().next().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5b08df-7b20-41f4-a8ff-112dface1cb0",
   "metadata": {
    "id": "9a5b08df-7b20-41f4-a8ff-112dface1cb0"
   },
   "source": [
    "# 3. Build Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f66add-a3db-467f-96c3-f87b9f880159",
   "metadata": {
    "id": "38f66add-a3db-467f-96c3-f87b9f880159"
   },
   "source": [
    "### 3.1 Import Modelling Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb72da39-377f-4264-b525-c87f49fb0356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in the sequential api for the generator and discriminator\n",
    "from tensorflow.keras.models import Sequential\n",
    "# Bring in the layers for the neural network\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40405df-1439-4661-8785-d76698df8152",
   "metadata": {
    "id": "c40405df-1439-4661-8785-d76698df8152"
   },
   "source": [
    "### 3.2 Build Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d29d43a-e02a-4031-a0ec-de8aa810c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(): \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Takes in random values and reshapes it to 7x7x128\n",
    "    # Beginnings of a generated image\n",
    "    model.add(Dense(7*7*128, input_dim=128))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Reshape((7,7,128)))\n",
    "    \n",
    "    # Upsampling block 1 \n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, 5, padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    # Upsampling block 2 \n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, 5, padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    # Convolutional block 1\n",
    "    model.add(Conv2D(128, 4, padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    # Convolutional block 2\n",
    "    model.add(Conv2D(128, 4, padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    # Conv layer to get to one channel\n",
    "    model.add(Conv2D(1, 4, padding='same', activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741b0d58-1b9f-4260-8405-dc400c73f843",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ab9c1-6d6c-49a0-b0c4-f45b7c68f588",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba4d1c-6a15-4097-bf63-5fe6ddb404b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = generator.predict(np.random.randn(4,128,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e0cb6-d741-4d43-b845-2a8f2615765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new fashion\n",
    "img = generator.predict(np.random.randn(4,128,1))\n",
    "# Setup the subplot formatting \n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "# Loop four times and get images \n",
    "for idx, img in enumerate(img): \n",
    "    # Plot the image using a specific subplot \n",
    "    ax[idx].imshow(np.squeeze(img))\n",
    "    # Appending the image label as the plot title \n",
    "    ax[idx].title.set_text(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2415abbf-24ed-4bac-8fb8-12c65017ec22",
   "metadata": {
    "id": "2415abbf-24ed-4bac-8fb8-12c65017ec22"
   },
   "source": [
    "### 3.3 Build Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e70bcb-cfd5-42bb-aed0-79f19bb38d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(): \n",
    "    model = Sequential()\n",
    "    \n",
    "    # First Conv Block\n",
    "    model.add(Conv2D(32, 5, input_shape = (28,28,1)))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    # Second Conv Block\n",
    "    model.add(Conv2D(64, 5))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    # Third Conv Block\n",
    "    model.add(Conv2D(128, 5))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    # Fourth Conv Block\n",
    "    model.add(Conv2D(256, 5))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    # Flatten then pass to dense layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7173eb57-250b-4d21-9b37-de842c4552ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = build_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6fecbc-f214-4f50-865c-91887b2430e7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e32424-f9c5-499c-a13f-b450bc525bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce3acc9-02c8-468f-915a-0efd52da0bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd15246-b40c-4c7a-912d-b88a1c5c463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.predict(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b343b0-38d3-4281-bedb-72099a18097e",
   "metadata": {
    "id": "39b343b0-38d3-4281-bedb-72099a18097e"
   },
   "source": [
    "# 4. Construct Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884abab3-2f74-442d-856f-e104ef1ac8ef",
   "metadata": {
    "id": "884abab3-2f74-442d-856f-e104ef1ac8ef"
   },
   "source": [
    "### 4.1 Setup Losses and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb1d23a-ea68-451a-bb38-e7795dc24311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam is going to be the optimizer for both\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Binary cross entropy is going to be the loss for both \n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198b2d4e-d6b9-4b6c-a98c-65cd1b81da26",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_opt = Adam(learning_rate=0.0001) \n",
    "d_opt = Adam(learning_rate=0.00001) \n",
    "g_loss = BinaryCrossentropy()\n",
    "d_loss = BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f170b0e-f731-4cbd-8068-24896f462c08",
   "metadata": {
    "id": "9f170b0e-f731-4cbd-8068-24896f462c08"
   },
   "source": [
    "### 4.2 Build Subclassed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2f5654-ed22-462d-be32-6c43d8b99b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the base model class to subclass our training step \n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a0af46-0243-4396-94d6-c1316d984de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionGAN(Model): \n",
    "    def __init__(self, generator, discriminator, *args, **kwargs):\n",
    "        # Pass through args and kwargs to base class \n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Create attributes for gen and disc\n",
    "        self.generator = generator \n",
    "        self.discriminator = discriminator \n",
    "        \n",
    "    def compile(self, g_opt, d_opt, g_loss, d_loss, *args, **kwargs): \n",
    "        # Compile with base class\n",
    "        super().compile(*args, **kwargs)\n",
    "        \n",
    "        # Create attributes for losses and optimizers\n",
    "        self.g_opt = g_opt\n",
    "        self.d_opt = d_opt\n",
    "        self.g_loss = g_loss\n",
    "        self.d_loss = d_loss \n",
    "\n",
    "    def train_step(self, batch):\n",
    "        # Get the data \n",
    "        real_images = batch\n",
    "        fake_images = self.generator(tf.random.normal((128, 128, 1)), training=False)\n",
    "        \n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as d_tape: \n",
    "            # Pass the real and fake images to the discriminator model\n",
    "            yhat_real = self.discriminator(real_images, training=True) \n",
    "            yhat_fake = self.discriminator(fake_images, training=True)\n",
    "            yhat_realfake = tf.concat([yhat_real, yhat_fake], axis=0)\n",
    "            \n",
    "            # Create labels for real and fakes images\n",
    "            y_realfake = tf.concat([tf.zeros_like(yhat_real), tf.ones_like(yhat_fake)], axis=0)\n",
    "            \n",
    "            # Add some noise to the TRUE outputs\n",
    "            noise_real = 0.15*tf.random.uniform(tf.shape(yhat_real))\n",
    "            noise_fake = -0.15*tf.random.uniform(tf.shape(yhat_fake))\n",
    "            y_realfake += tf.concat([noise_real, noise_fake], axis=0)\n",
    "            \n",
    "            # Calculate loss - BINARYCROSS \n",
    "            total_d_loss = self.d_loss(y_realfake, yhat_realfake)\n",
    "            \n",
    "        # Apply backpropagation - nn learn \n",
    "        dgrad = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables) \n",
    "        self.d_opt.apply_gradients(zip(dgrad, self.discriminator.trainable_variables))\n",
    "        \n",
    "        # Train the generator \n",
    "        with tf.GradientTape() as g_tape: \n",
    "            # Generate some new images\n",
    "            gen_images = self.generator(tf.random.normal((128,128,1)), training=True)\n",
    "                                        \n",
    "            # Create the predicted labels\n",
    "            predicted_labels = self.discriminator(gen_images, training=False)\n",
    "                                        \n",
    "            # Calculate loss - trick to training to fake out the discriminator\n",
    "            total_g_loss = self.g_loss(tf.zeros_like(predicted_labels), predicted_labels) \n",
    "            \n",
    "        # Apply backprop\n",
    "        ggrad = g_tape.gradient(total_g_loss, self.generator.trainable_variables)\n",
    "        self.g_opt.apply_gradients(zip(ggrad, self.generator.trainable_variables))\n",
    "        \n",
    "        return {\"d_loss\":total_d_loss, \"g_loss\":total_g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d248c3-f4c1-4478-a699-a5811a7b1fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of subclassed model\n",
    "fashgan = FashionGAN(generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf7e02-ee1a-4901-bdf0-9aa2301f8cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "fashgan.compile(g_opt, d_opt, g_loss, d_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06d0adb-38d0-4558-b824-7416cf880082",
   "metadata": {
    "id": "e06d0adb-38d0-4558-b824-7416cf880082"
   },
   "source": [
    "### 4.3 Build Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f6918-366c-4799-9dac-1acedaab40c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e2bb77-2d7d-40d0-809f-526b8fd34170",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMonitor(Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.uniform((self.num_img, self.latent_dim,1))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = array_to_img(generated_images[i])\n",
    "            img.save(os.path.join('images', f'generated_img_{epoch}_{i}.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e2f159-25e7-4e35-95ef-f0fd18ac5897",
   "metadata": {
    "id": "16e2f159-25e7-4e35-95ef-f0fd18ac5897"
   },
   "source": [
    "### 4.3 Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a779dceb-aba6-4bf3-af49-0d32a76dd2f7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recommend 2000 epochs\n",
    "hist = fashgan.fit(ds, epochs=20, callbacks=[ModelMonitor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c665a1-a4cc-41ac-a08a-2e14ba64e88d",
   "metadata": {
    "id": "39c665a1-a4cc-41ac-a08a-2e14ba64e88d"
   },
   "source": [
    "### 4.4 Review Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54381e8c-93ee-4022-9df6-24c4356720fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.suptitle('Loss')\n",
    "plt.plot(hist.history['d_loss'], label='d_loss')\n",
    "plt.plot(hist.history['g_loss'], label='g_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d319a982-7ae5-4754-adcf-b490f17a79d6",
   "metadata": {
    "id": "d319a982-7ae5-4754-adcf-b490f17a79d6"
   },
   "source": [
    "# 5. Test Out the Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ba81f-978a-4c31-9c3d-6ebe5a5bfc29",
   "metadata": {
    "id": "206ba81f-978a-4c31-9c3d-6ebe5a5bfc29"
   },
   "source": [
    "### 5.1 Generate Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46f3d6a-8aa5-40d2-a5ac-67a0606a82f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.load_weights(os.path.join('archive', 'generatormodel.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cde11f-cb26-4ebf-ad04-2c64a54f871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = generator.predict(tf.random.normal((16, 128, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f745982f-c4d7-451f-91a7-f7c4341cb7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, nrows=4, figsize=(10,10))\n",
    "for r in range(4): \n",
    "    for c in range(4): \n",
    "        ax[r][c].imshow(imgs[(r+1)*(c+1)-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5137cffa-784d-4076-beef-0a067b86d3aa",
   "metadata": {
    "id": "5137cffa-784d-4076-beef-0a067b86d3aa"
   },
   "source": [
    "### 5.2 Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7011d68-ef71-4377-91e2-e26a02fab382",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save('generator.h5')\n",
    "discriminator.save('discriminator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c2bd3-a344-4ac1-b2ee-6c90420368e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [
    "206ba81f-978a-4c31-9c3d-6ebe5a5bfc29"
   ],
   "name": "FashionGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
